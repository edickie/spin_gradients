{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in the HCP data to make template gradients, a file containing ROIs with their associated networks, and the margulies \n",
    "gradients. Margulies gradients are transposed and selected for the first 10 o be aligned with the HCP data. \n",
    "ROIs are added to the HCP gradients file and the network file is loaded, so the two can be merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pconnGrad = '/scratch/a/arisvoin/edickie/SPINS_glasser_tian/group-HCPS1200_atlas-GlasserTian_desc-subcorticalS2_conn.pconn.nii'\n",
    "networks = '/scratch/a/arisvoin/lbassman/spins_gradients/networks.txt'\n",
    "margulies = '/scratch/a/arisvoin/edickie/SPINS_glasser_tian/tpl-fsLR_den-32k_atlas-Glasser2016Tian2019S2_desc-margulies2016_gradients.pscalar.nii'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grad1</th>\n",
       "      <th>grad2</th>\n",
       "      <th>grad3</th>\n",
       "      <th>grad4</th>\n",
       "      <th>grad5</th>\n",
       "      <th>grad6</th>\n",
       "      <th>grad7</th>\n",
       "      <th>grad8</th>\n",
       "      <th>grad9</th>\n",
       "      <th>grad10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.356939</td>\n",
       "      <td>-0.644287</td>\n",
       "      <td>-2.367967</td>\n",
       "      <td>-0.216552</td>\n",
       "      <td>0.391726</td>\n",
       "      <td>0.075548</td>\n",
       "      <td>0.604169</td>\n",
       "      <td>0.375747</td>\n",
       "      <td>-0.391762</td>\n",
       "      <td>0.040092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.631859</td>\n",
       "      <td>-0.493243</td>\n",
       "      <td>-1.348967</td>\n",
       "      <td>0.186536</td>\n",
       "      <td>0.037536</td>\n",
       "      <td>-0.065967</td>\n",
       "      <td>0.543003</td>\n",
       "      <td>0.215190</td>\n",
       "      <td>-0.183392</td>\n",
       "      <td>-0.078787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.720141</td>\n",
       "      <td>-0.685711</td>\n",
       "      <td>-2.397643</td>\n",
       "      <td>-0.434602</td>\n",
       "      <td>0.792402</td>\n",
       "      <td>-0.957256</td>\n",
       "      <td>-0.182126</td>\n",
       "      <td>0.363215</td>\n",
       "      <td>-0.479404</td>\n",
       "      <td>0.119942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.203116</td>\n",
       "      <td>-1.047820</td>\n",
       "      <td>-2.399251</td>\n",
       "      <td>-0.415645</td>\n",
       "      <td>0.569269</td>\n",
       "      <td>-0.545615</td>\n",
       "      <td>0.010176</td>\n",
       "      <td>0.360974</td>\n",
       "      <td>-0.461716</td>\n",
       "      <td>0.126106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.799218</td>\n",
       "      <td>0.695928</td>\n",
       "      <td>-0.428229</td>\n",
       "      <td>0.528345</td>\n",
       "      <td>-0.209360</td>\n",
       "      <td>-0.129425</td>\n",
       "      <td>0.307516</td>\n",
       "      <td>-0.008360</td>\n",
       "      <td>0.074665</td>\n",
       "      <td>-0.012762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>5.763892</td>\n",
       "      <td>-0.151041</td>\n",
       "      <td>-2.701267</td>\n",
       "      <td>-0.028442</td>\n",
       "      <td>1.065733</td>\n",
       "      <td>0.229579</td>\n",
       "      <td>-0.138070</td>\n",
       "      <td>-0.000859</td>\n",
       "      <td>-0.016951</td>\n",
       "      <td>0.155254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>6.399410</td>\n",
       "      <td>-0.093912</td>\n",
       "      <td>-0.684766</td>\n",
       "      <td>-0.312900</td>\n",
       "      <td>-0.617976</td>\n",
       "      <td>0.083697</td>\n",
       "      <td>-0.014652</td>\n",
       "      <td>-0.236935</td>\n",
       "      <td>0.337544</td>\n",
       "      <td>-0.215534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>-2.662942</td>\n",
       "      <td>-1.026617</td>\n",
       "      <td>-0.145998</td>\n",
       "      <td>0.505102</td>\n",
       "      <td>0.865369</td>\n",
       "      <td>-0.730838</td>\n",
       "      <td>0.159122</td>\n",
       "      <td>-0.022565</td>\n",
       "      <td>0.099260</td>\n",
       "      <td>-0.407851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>0.471279</td>\n",
       "      <td>-0.391846</td>\n",
       "      <td>1.641826</td>\n",
       "      <td>0.658301</td>\n",
       "      <td>0.418799</td>\n",
       "      <td>-0.029401</td>\n",
       "      <td>0.178059</td>\n",
       "      <td>-0.084444</td>\n",
       "      <td>0.059446</td>\n",
       "      <td>0.364249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>0.513285</td>\n",
       "      <td>-0.461504</td>\n",
       "      <td>1.296258</td>\n",
       "      <td>0.882958</td>\n",
       "      <td>0.390704</td>\n",
       "      <td>0.099377</td>\n",
       "      <td>0.291650</td>\n",
       "      <td>-0.020811</td>\n",
       "      <td>-0.016611</td>\n",
       "      <td>0.323056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>392 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        grad1     grad2     grad3     grad4     grad5     grad6     grad7  \\\n",
       "0    1.356939 -0.644287 -2.367967 -0.216552  0.391726  0.075548  0.604169   \n",
       "1   -0.631859 -0.493243 -1.348967  0.186536  0.037536 -0.065967  0.543003   \n",
       "2   -0.720141 -0.685711 -2.397643 -0.434602  0.792402 -0.957256 -0.182126   \n",
       "3   -0.203116 -1.047820 -2.399251 -0.415645  0.569269 -0.545615  0.010176   \n",
       "4   -2.799218  0.695928 -0.428229  0.528345 -0.209360 -0.129425  0.307516   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "387  5.763892 -0.151041 -2.701267 -0.028442  1.065733  0.229579 -0.138070   \n",
       "388  6.399410 -0.093912 -0.684766 -0.312900 -0.617976  0.083697 -0.014652   \n",
       "389 -2.662942 -1.026617 -0.145998  0.505102  0.865369 -0.730838  0.159122   \n",
       "390  0.471279 -0.391846  1.641826  0.658301  0.418799 -0.029401  0.178059   \n",
       "391  0.513285 -0.461504  1.296258  0.882958  0.390704  0.099377  0.291650   \n",
       "\n",
       "        grad8     grad9    grad10  \n",
       "0    0.375747 -0.391762  0.040092  \n",
       "1    0.215190 -0.183392 -0.078787  \n",
       "2    0.363215 -0.479404  0.119942  \n",
       "3    0.360974 -0.461716  0.126106  \n",
       "4   -0.008360  0.074665 -0.012762  \n",
       "..        ...       ...       ...  \n",
       "387 -0.000859 -0.016951  0.155254  \n",
       "388 -0.236935  0.337544 -0.215534  \n",
       "389 -0.022565  0.099260 -0.407851  \n",
       "390 -0.084444  0.059446  0.364249  \n",
       "391 -0.020811 -0.016611  0.323056  \n",
       "\n",
       "[392 rows x 10 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marg = nib.load(margulies)\n",
    "mar = marg.get_fdata().T\n",
    "margulies_df = pd.DataFrame(data = mar, \n",
    "    columns=[f'grad{num + 1}' for num in range(mar.shape[1])])\n",
    "margulies_grad_df = margulies_df[['grad1','grad2','grad3','grad4','grad5','grad6','grad7','grad8','grad9','grad10']] #change this\n",
    "margulies_grad_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ROIs_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-82b83f204eee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnetwork_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetworks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnetworks_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgrad_roi_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrad_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mROIs_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ROIs_list' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "network_file = pd.read_csv(networks, sep=\"\\t\") \n",
    "networks_df = pd.DataFrame(data = network_file)\n",
    "grad_roi_df = pd.concat([grad_df, ROIs_list], axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the network file and the HCP gradients containing the ROI names to have network values for each ROI. Making\n",
    "it into a df as well to be added to the all_concat file below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_networks = pd.merge(grad_roi_df, networks_df, how = \"left\", left_on = \"ROI\", right_on = \"GLASSERLABELNAME\")\n",
    "merge_networks['network1'] = merge_networks['NETWORK'].replace(np.nan, \"Subcortical\")\n",
    "networks_list = merge_networks['network1'].to_numpy()\n",
    "\n",
    "networks_list_df = pd.DataFrame(networks_list, \n",
    "                         columns=['Network'])\n",
    "path = '/scratch/a/arisvoin/lbassman/spins_gradients/merge_networks'    #writes dataframe to csv in scratch \n",
    "\n",
    "merge_networks.to_csv(path_or_buf=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create_gradient functions takes in a nifty file type and creates a pandas dataframe containing the gradient values, \n",
    "the size will be the number of ROIs present in the nifty files by the number  of gradients desired. grad_df is \n",
    "the calculated gradients for the HCP data, and 10 gradients are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a/arisvoin/lbassman/.conda/envs/nilearn_brainspace/lib/python3.8/site-packages/brainspace/gradient/embedding.py:70: UserWarning: Affinity is not symmetric. Making symmetric.\n",
      "  warnings.warn('Affinity is not symmetric. Making symmetric.')\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "from brainspace.gradient import GradientMaps\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def create_gradient(nifty, n, z_to_r):\n",
    "    '''Create a pandas dataframe of n number of gradients from nifty file\n",
    "\n",
    "    Loads .nii file with nibabel, creates a float type matrix\n",
    "    Loads GradientMaps, and fits to the matrix\n",
    "    Converts matrix to a pandas dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nifty: .nii file type\n",
    "        nifty file that will be read to create the gradients\n",
    "    n: int\n",
    "        The number of gradients to be created\n",
    "    z_to_r: bool\n",
    "        Determines whether hyperbolic tan is applied to the connectivity matrix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    grad_df\n",
    "        An array of values, rows x columns is ROIs x n number of gradients \n",
    "\n",
    "    ''' \n",
    "\n",
    "    # load the nii file with nibabel\n",
    "    conn = nib.load(nifty)\n",
    "    # create matrix with float data\n",
    "    matrix = conn.get_fdata()\n",
    "    # specify whether matrix converted to tanh matrix is true or false\n",
    "    if z_to_r:\n",
    "        matrix = np.tanh(matrix)\n",
    "    # load GradientMaps\n",
    "    gm = GradientMaps(n_components=n, random_state=0)\n",
    "    # assert that the number of gradients requested does not exceed the regions of interest \n",
    "    max_grads = matrix.shape[0]\n",
    "    assert n <= max_grads, 'Number of gradients exceeds rows in the matrix'\n",
    "    # create the gradient as a matrix\n",
    "    gm.fit(matrix)\n",
    "    gradient = gm.gradients_\n",
    "    # convert gradient to a pandas dataframe\n",
    "    grad_df = pd.DataFrame(data = gradient, \n",
    "    columns=[f'grad{num + 1}' for num in range(gm.gradients_.shape[1])])\n",
    "    return grad_df\n",
    "\n",
    "grad_df = create_gradient(pconnGrad, 10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the HCP data as a float matrix to be used later in the calc_aligned_gradients function (deconstructed calc_gradient function to get timeseries in lieu of gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = nib.load(pconnGrad)\n",
    "matrix = conn.get_fdata()\n",
    "grad_matrix_df = pd.DataFrame(data = matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write_df_column_to_pscalar_nib function takes the gradients that were created and converts them to a pscalar file\n",
    "which allows for the gradient to be plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def write_df_column_to_pscalar_nib(df, pscalar_template, \n",
    "        to_filename = None, columns = None, labelname_column = None):\n",
    "    ''' write a column from a pandas dataframe to pscalar file in nibabel \n",
    "     Parameters\n",
    "    ----------\n",
    "    df: pandas DataFrame\n",
    "        Pandas dataframe containing data to write to file (with rows as parcels)\n",
    "    \n",
    "    pscalar_template: nibabel.Cifti2Image or filepath\n",
    "        A pscalar.nii or pterseries.nii file or image to read the parcel axis from\n",
    "    \n",
    "    to_filename: str or path\n",
    "        (optional) path (with extension .pscalar.nii) to write the output to\n",
    "    columns: str or list\n",
    "        (optional) list of columns to select for the pscalar image\n",
    "    labelnames_column: str\n",
    "        (optional) name of column that contains labelnames that should match the pscalar parcel names\n",
    "        If this is given than data will be merged/aligned with the parcel axis before writing file.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pscalar_img: nibabel.Cifti2Image\n",
    "        nibabel image of the pscalar output\n",
    "        '''\n",
    "           ## read the parcel axis from a template file\n",
    "    if isinstance(pscalar_template, (str, Path)):\n",
    "        pscalar_template = nib.load(pscalar_template)\n",
    "    template_axis = pscalar_template.header.get_index_map(1)\n",
    "    axis1_parcels = nib.cifti2.cifti2_axes.ParcelsAxis.from_index_mapping(template_axis)\n",
    "    assert (isinstance(axis1_parcels, nib.cifti2.cifti2_axes.ParcelsAxis)), \"Did not creat parcel axis\"\n",
    "\n",
    "    if labelname_column:\n",
    "        axis_df = pd.DataFrame({'labelnames': axis1_parcels.name})\n",
    "        df = df.merge(axis_df, \n",
    "            left_on = labelname_column, right_on = 'labelnames', how = 'right')\n",
    "    \n",
    "    ## grab columns if they exist\n",
    "    if columns:\n",
    "        df = df[columns]\n",
    "\n",
    "    # grab the data column(s) and resphape them to the shape for pscalar \n",
    "    ## note that the dataobj shape should be (n_map, n_parcels)\n",
    "    data_vals = df.transpose().values\n",
    "    if len(data_vals.shape) == 1:\n",
    "        data_vals = np.reshape(data_vals, (1,data_vals.shape[0]))\n",
    "    \n",
    "    ## assert that the number of parcels matches the length of the data array\n",
    "    assert (axis1_parcels.size == len(df.index)), 'The number of parcels does not match the number of rows'\n",
    "\n",
    "    ## create a scalar axis with names inlcuding the column names\n",
    "    axis0_scalars = nib.cifti2.cifti2_axes.ScalarAxis(name = df.columns)\n",
    "    \n",
    "    ## combine all the bits together\n",
    "    new_header = nib.Cifti2Header.from_axes((axis0_scalars, axis1_parcels))\n",
    "    pscalar_img = nib.Cifti2Image(\n",
    "        dataobj=data_vals, header = new_header)\n",
    "    if to_filename:\n",
    "        nib.cifti2.save(\n",
    "            img = pscalar_img,filename=to_filename)\n",
    "\n",
    "    return(pscalar_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are implemented into a final function: plot_cifti_surf_montage which takes in the pscalar from the previous\n",
    "function and plots the gradients. Template maps are used, and any matplotlib colormap can be used. Four plots total will be created\n",
    "for each gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a/arisvoin/lbassman/.conda/envs/nilearn_brainspace/lib/python3.8/site-packages/nilearn/datasets/__init__.py:87: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  warn(\"Fetchers from the nilearn.datasets module will be \"\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "colormap_from_ciftiheader() missing 1 required positional argument: 'dlabel_img'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-3411ab191909>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;31m# %%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mListedColormap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolormap_from_ciftiheader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;31m# %%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: colormap_from_ciftiheader() missing 1 required positional argument: 'dlabel_img'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from enum import Enum\n",
    "import nilearn.plotting as nplot\n",
    "import nibabel as nib\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "CIFTI_GIFTI_MAP = {\n",
    "    'CIFTI_STRUCTURE_CORTEX_LEFT': 'left',\n",
    "    'CIFTI_STRUCTURE_CORTEX_RIGHT': 'right',\n",
    "    'CortexLeft': 'left',\n",
    "    'CortexRight': 'right'\n",
    "}\n",
    "\n",
    "NIFTI_INTENT_POINTSET = 1008\n",
    "\n",
    "\n",
    "def gifti_get_mesh(gifti):\n",
    "    '''\n",
    "    Extract vertices and triangles from GIFTI surf.gii\n",
    "    file\n",
    "    Arguments:\n",
    "        gifti (GiftiImage): Input GiftiImage\n",
    "    '''\n",
    "\n",
    "    v, t = gifti.agg_data(('pointset', 'triangle'))\n",
    "    return v.copy(), t.copy()\n",
    "\n",
    "def map_cifti_to_gifti(gifti, cifti, cifti_map_index = 0, fill_value = np.nan):\n",
    "    '''\n",
    "    Maps cifti data-array to gifti vertices to account\n",
    "    for missing indices (i.e removal of medial wall)\n",
    "    Arguments:\n",
    "        gifti:      GIFTI surface mesh\n",
    "        cifti:      CIFTI file to map [Series x BrainModel]\n",
    "    Returns:\n",
    "        verts:          Vertices of surface mesh\n",
    "        trigs:          Triangles of surface mesh\n",
    "        mapping_array:  An [Features x Vertices] mapping array pulled\n",
    "                        from the CIFTI image\n",
    "    '''\n",
    "\n",
    "    # Validate and obtain CIFTI indices\n",
    "    brain_models = None\n",
    "    parcels = None\n",
    "    for mi in cifti.header.mapped_indices:\n",
    "        map_type = cifti.header.get_index_map(mi).indices_map_to_data_type\n",
    "        if map_type == \"CIFTI_INDEX_TYPE_BRAIN_MODELS\":\n",
    "            brain_models = cifti.header.get_axis(mi)\n",
    "        if map_type == \"CIFTI_INDEX_TYPE_PARCELS\":\n",
    "            parcels = cifti.header.get_axis(mi)\n",
    "\n",
    "    # TODO: Implement logging + proper error\n",
    "    if (brain_models is None) and (parcels is None):\n",
    "        raise ValueError(\"CIFTI object does not contain BrainModelAxis!\")\n",
    "\n",
    "    # Validate and obtain GIFTI\n",
    "    gifti_struct = None\n",
    "    for d in gifti.darrays:\n",
    "        if d.intent == NIFTI_INTENT_POINTSET:\n",
    "            try:\n",
    "                gifti_struct = d.metadata['AnatomicalStructurePrimary']\n",
    "            except KeyError:\n",
    "                raise ValueError(\n",
    "                    f\"{gifti.get_filename()} is not a surface mesh file!\")\n",
    "\n",
    "    if gifti_struct is None:\n",
    "        raise ValueError(\n",
    "            f\"{gifti.get_filename()} contains no coordinate information!\")\n",
    "\n",
    "    # Now we need to map the coordinate of CIFTI onto GIFTI\n",
    "    match_key = CIFTI_GIFTI_MAP[gifti_struct]\n",
    "    \n",
    "    # Extract vertices from GIFTI\n",
    "    verts, trigs = gifti_get_mesh(gifti)\n",
    "    \n",
    "    for stucture in ['CIFTI_STRUCTURE_CORTEX_LEFT', 'CIFTI_STRUCTURE_CORTEX_RIGHT']:\n",
    "        if CIFTI_GIFTI_MAP[stucture] == match_key:\n",
    "            if parcels:\n",
    "                mapping_array = surf_map_from_pscalar_image(cifti, \n",
    "                                                            stucture, \n",
    "                                                            map = cifti_map_index, \n",
    "                                                            fill_value = fill_value)\n",
    "            if brain_models:\n",
    "                mapping_array = surf_data_from_dense_cifti(cifti, \n",
    "                                                          stucture,\n",
    "                                                          map_index = cifti_map_index,\n",
    "                                                          fill_value = fill_value)\n",
    "                \n",
    "    \n",
    "#     matched_bm = None\n",
    "#     for struct, sl, bma in brain_models.iter_structures():\n",
    "#         if CIFTI_GIFTI_MAP[struct] == match_key:\n",
    "#             matched_bm = (struct, sl, bma)\n",
    "#             break       \n",
    "            \n",
    "#     if matched_bm is None:\n",
    "#         raise ValueError(\n",
    "#             \"No matching structures between CIFTI and GIFTI file!\")\n",
    "\n",
    "#     _, matched_verts, brain_model_ax = matched_bm\n",
    "#     cifti_verts = brain_model_ax.vertex\n",
    "\n",
    "#     # Map CIFTI vertices to GIFTI, setting non-filled values to NaN\n",
    "#     mapping_array = np.empty((cifti.dataobj.shape[0], verts.shape[0]),\n",
    "#                              dtype=cifti.dataobj.dtype)\n",
    "\n",
    "#     # Write NaNs\n",
    "#     mapping_array[:] = np.nan\n",
    "#     try:\n",
    "#         mapping_array[:, cifti_verts] = cifti.get_fdata()[:, matched_verts]\n",
    "#     except IndexError:\n",
    "#         raise ValueError(\"Cifti file contains vertices that are not indexed \"\n",
    "#                          \"by the provided gifti file!\")\n",
    "\n",
    "    # Return mapping array\n",
    "    return verts, trigs, mapping_array\n",
    "\n",
    "\n",
    "def surf_data_from_dense_cifti(cifti, surf_name, map_index = \"all\", fill_value = np.nan):\n",
    "    # the dscalar img is a filepath - load it\n",
    "    if isinstance(cifti, (str, Path)):\n",
    "        cifti = nib.load(cifti)\n",
    "    axis = cifti.header.get_axis(1)\n",
    "    data = cifti.get_fdata()\n",
    "    if not map_index == \"all\":\n",
    "        data = data[map_index,:]\n",
    "    assert isinstance(axis, nib.cifti2.BrainModelAxis)\n",
    "    for name, data_indices, model in axis.iter_structures():  # Iterates over volumetric and surface structures\n",
    "        if name == surf_name:                                 # Just looking for a surface\n",
    "            data = data.T[data_indices]                       # Assume brainmodels axis is last, move it to front\n",
    "            vtx_indices = model.vertex                        # Generally 1-N, except medial wall vertices\n",
    "            surf_data = np.zeros((vtx_indices.max() + 1,) + data.shape[1:], dtype=data.dtype)\n",
    "            surf_data[:] = fill_value\n",
    "            surf_data[vtx_indices] = data\n",
    "            return surf_data\n",
    "    raise ValueError(f\"No structure named {surf_name}\")\n",
    "\n",
    "\n",
    "pconn = '/scratch/a/arisvoin/edickie/SPINS_glasser_tian/group-HCPS1200_atlas-GlasserTian_desc-subcorticalS2_conn.pconn.nii'\n",
    "\n",
    "\n",
    "def surf_map_from_pscalar_image(pscalar_img, surface, map = 0, fill_value = np.nan):\n",
    "    '''\n",
    "    Read data from parcellated scalar or parcellated timeseries image into full surface map for plotting\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pscalar_img: nibabel image, str or path\n",
    "        nibabel image or filepath to image with extension .pscalar.nii\n",
    "    \n",
    "    surface: str\n",
    "        Surface to load (either CIFTI_STRUCTURE_CORTEX_LEFT or CIFTI_STRUCTURE_CORTEX_RIGHT)\n",
    "    \n",
    "    map: str or integer\n",
    "        Either the name of the scalar map or the column index (default: 0)\n",
    "    fill_value: numeric value\n",
    "        value to fill missing data non-parcel vertices on the surfuce (default: np.nan)\n",
    "    Return\n",
    "    ------\n",
    "    surf_out: numpy data array (nvertices,)\n",
    "        the parcellated scalar data mapped to the surface\n",
    "    '''\n",
    "    \n",
    "    # the pscalar img is a filepath - load it\n",
    "    if isinstance(pscalar_img, (str, Path)):\n",
    "        pscalar_img = nib.load(pscalar_img)\n",
    "    \n",
    "    ## define map_number based on the \"map\" input arg\n",
    "    if isinstance(map, str):\n",
    "        if map == \"all\":\n",
    "            map_number = list(range(len(pscalar_img.header.get_axis(1).name)))\n",
    "        else:\n",
    "            map_number = np.where(pscalar_img.header.get_axis(0).name==map)[0]\n",
    "            if len(map_number) != 1:\n",
    "                raise ValueError(f\"the map name {map} was not present in the scalar axis names\")\n",
    "            map_number = int(map_number[0])\n",
    "    else: \n",
    "        map_number = int(map)\n",
    "\n",
    "    # grab the data\n",
    "    data = pscalar_img.get_fdata()[map_number,:]\n",
    "    if len(data.shape) == 1:\n",
    "      data = data.reshape(data.shape[0],1)\n",
    "\n",
    "    # grab the parcel axis (axis 1)\n",
    "    axis = pscalar_img.header.get_axis(1)\n",
    "    assert isinstance(axis, nib.cifti2.ParcelsAxis)\n",
    "\n",
    "    # intialize an output array\n",
    "    surf_out = np.zeros((axis.nvertices[surface], data.shape[1]))\n",
    "    surf_out[:] = fill_value\n",
    "\n",
    "    # iterate over the parcels writing the parcel values to the vertex indices\n",
    "    for (parcel_data, parcel_vertices) in zip(data, axis.vertices):\n",
    "        if surface in parcel_vertices:\n",
    "            surf_out[parcel_vertices[surface],:] = parcel_data\n",
    "    \n",
    "    return surf_out\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "def colormap_from_ciftiheader(dlabel_img):\n",
    "    ''' reads the colormap from the cifti image LabelAxis for dense labels files (dlabel.nii)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dlabel_image: str, path or nibabel.cifti2 image\n",
    "        input label file or image\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    cm : matplotlib colormap\n",
    "        matplotlib colormap object read with the color look up table colours\n",
    "    '''\n",
    "    if isinstance(dlabel_img, (str, Path)):\n",
    "            dlabel_img = nib.load(dlabel_img)\n",
    "        # read the labeltable from the header\n",
    "    label_axis = dlabel_img.header.get_axis(0)\n",
    "    assert isinstance(label_axis, nib.cifti2.cifti2_axes.LabelAxis)\n",
    "    label_colours = [val[1] for val in label_axis.label[0].values()]\n",
    "    cm = ListedColormap(\n",
    "        name = label_axis.name[0], \n",
    "        colors = label_colours, \n",
    "        N=max(label_axis.label[0].keys())+1)\n",
    "    return cm\n",
    "\n",
    "    \n",
    "def plot_cifti_surf_montage(left_surf, right_surf, cifti_map, \n",
    "                            bg_cifti_map = None, \n",
    "                            views = [\"lateral\", \"medial\", \"medial\", \"lateral\"],\n",
    "                            hemis = [\"left\", \"left\", \"right\", \"right\"],\n",
    "                            bg_cifti_map_index = \"all\", \n",
    "                            colormap = None, \n",
    "                            darkness = 0.8,\n",
    "                            add_view_titles = True,\n",
    "                            to_filename = None):\n",
    "    \"\"\"\n",
    "    plotting montage of both hemispheres\n",
    "    \n",
    "    Generate a surface visualization\n",
    "    Parameters\n",
    "    --------\n",
    "    left_surf: str or Path\n",
    "      \n",
    "    bg_cifti_map :\n",
    "      (optional) Cifti file containing background map data (usually sulci depth)\n",
    "    Returns:\n",
    "        runtime: Resultant runtime object\n",
    "    \"\"\"\n",
    "\n",
    "    from mpl_toolkits import mplot3d  # noqa: F401\n",
    "\n",
    "    assert len(views) == len(hemis), f\"number of views ({views}) is not equal to the number of hemipheres ({hemis})\"\n",
    "        \n",
    "    Hemispheres = namedtuple(\"Hemispheres\", [\"left\", \"right\"])\n",
    "    vmin = None\n",
    "    vmax = None\n",
    "\n",
    "    l_surf = nib.load(left_surf)\n",
    "    r_surf = nib.load(right_surf)\n",
    "    num_views = len(views)\n",
    "    num_maps = 1\n",
    "\n",
    "    if cifti_map:\n",
    "        cifti_map = nib.load(cifti_map)\n",
    "        lv, lt, lm = map_cifti_to_gifti(l_surf, cifti_map, cifti_map_index = bg_cifti_map_index)\n",
    "        rv, rt, rm = map_cifti_to_gifti(r_surf, cifti_map, cifti_map_index = bg_cifti_map_index)\n",
    "\n",
    "        map_hemi = Hemispheres(left=(lv, lt, lm), right=(rv, rt, rm))\n",
    "        num_maps = lm.shape[1]\n",
    "        \n",
    "        if colormap == \"read_from_cifti\":\n",
    "            colormap = colormap_from_ciftiheader(cifti_map)\n",
    "        else:\n",
    "            vmin, vmax = np.nanpercentile(cifti_map.get_fdata(), [2, 98])\n",
    "    else:\n",
    "        # Use vertices and triangles from Mesh\n",
    "        lv, lt = gifti_get_mesh(l_surf)\n",
    "        rv, rt = gifti_get_mesh(r_surf)\n",
    "        map_hemi = Hemispheres(left=(lv, lt, None), right=(rv, rt, None))\n",
    "        \n",
    "\n",
    "    if bg_cifti_map:\n",
    "        bg_map = nib.load(bg_cifti_map)\n",
    "        _, _, l_bg = map_cifti_to_gifti(l_surf, bg_map, fill_value = 0)\n",
    "        _, _, r_bg = map_cifti_to_gifti(r_surf, bg_map, fill_value = 0)\n",
    "        bg_hemi = Hemispheres(left=l_bg, right=r_bg)\n",
    "    else:\n",
    "        bg_hemi = Hemispheres(left=None, right=None)\n",
    "\n",
    "    # Construct figure\n",
    "    w, h = plt.figaspect(num_maps / (num_views))\n",
    "    fig, axs = plt.subplots(num_maps,\n",
    "                            num_views,\n",
    "                            subplot_kw={'projection': '3d'},\n",
    "                            figsize=(w, h))\n",
    "    fig.set_facecolor(\"white\")\n",
    "    fig.tight_layout()\n",
    "    for i, a in enumerate(axs.flat):\n",
    "        a.set_facecolor(\"white\")\n",
    "\n",
    "        # Get row (map)\n",
    "        i_map = i // (num_views)\n",
    "\n",
    "        # Get column\n",
    "        i_view = (i) % (num_views)\n",
    "        view = views[i_view]\n",
    "\n",
    "        # Get hemisphere\n",
    "        hemi = hemis[i_view]\n",
    "        if hemi == \"left\":\n",
    "            display_map = map_hemi.left\n",
    "            display_bg = bg_hemi.left\n",
    "        elif hemi == \"right\":\n",
    "            display_map = map_hemi.right\n",
    "            display_bg = bg_hemi.right\n",
    "        else:\n",
    "            raise ValueError('hemis must be \"left\" or \"right\"')\n",
    "        \n",
    "        if add_view_titles: a.set_title(f'{hemi} {view}')\n",
    "        # Plot\n",
    "        v, t, m = display_map\n",
    "        nplot.plot_surf([v, t],\n",
    "                        surf_map=m,\n",
    "                        bg_map= display_bg,\n",
    "                        cmap=colormap,\n",
    "                        axes=a,\n",
    "                        hemi=hemi,\n",
    "                        view=view,\n",
    "                        bg_on_data=True,\n",
    "                        vmin = vmin,\n",
    "                        vmax = vmax,\n",
    "                        darkness=darkness)\n",
    "    \n",
    "    if not to_filename:    \n",
    "        plt.draw()\n",
    "    else:\n",
    "        plt.savefig(to_filename)\n",
    "    \n",
    "    return(plt)\n",
    "\n",
    "\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "\n",
    "dlabel_file = '/scratch/a/arisvoin/edickie/SPINS_glasser_tian/tpl-fsLR_den-32k_atlas-Glasser2016Tian2019_desc-subcortexS2_dseg.dlabel.nii'\n",
    "\n",
    "\n",
    "\n",
    "label_axis = nib.load(dlabel_file).header.get_axis(0)\n",
    "label_colours = [val[1] for val in label_axis.label[0].values()]\n",
    "label_keys = label_axis.label[0].keys()\n",
    "\n",
    "\n",
    "max(label_keys)\n",
    "\n",
    "# %%\n",
    "from matplotlib.colors import ListedColormap\n",
    "cm = colormap_from_ciftiheader()\n",
    "\n",
    "# %%\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.array(list(label_keys))\n",
    "y = np.ones(len(label_colours)) \n",
    "\n",
    "plt.figure(figsize=(20,1))\n",
    "plt.scatter(x, y, marker='o', s = 100, c=x, cmap = cm)\n",
    "\n",
    "# Show the boundary between the regions:\n",
    "plt.show()\n",
    "\n",
    "\n",
    "lv, lt, lm = map_cifti_to_gifti(nib.load('/scratch/a/arisvoin/edickie/SPINS_glasser_tian/tpl-fsLR/S1200.L.inflated_MSMAll.32k_fs_LR.surf.gii'), \n",
    "                                nib.load(dlabel_file), cifti_map_index = \"all\")\n",
    "\n",
    "\n",
    "label_axis.name[0]\n",
    "\n",
    "\n",
    "yeo_cm = colormap_from_ciftiheader('/scratch/a/arisvoin/edickie/SPINS_glasser_tian/tpl-fsLR_den-32k_atlas-Ji2019_desc-12net_dseg.dlabel.nii')\n",
    "x = np.array(range(14))\n",
    "y = np.ones(14) \n",
    "\n",
    "plt.figure(figsize=(20,1))\n",
    "plt.scatter(x, y, marker='o', s = 100, c=x, cmap = yeo_cm)\n",
    "\n",
    "# Show the boundary between the regions:\n",
    "plt.show()\n",
    "\n",
    "\n",
    "surf_map_from_pscalar_image(pconn, \"CIFTI_STRUCTURE_CORTEX_LEFT\", map = 'aHIP-rh', fill_value = np.nan).shape\n",
    "\n",
    "\n",
    "\n",
    "cifti_map = 'HCP_grads.pscalar.nii'\n",
    "dcifti = nib.load(cifti_map)\n",
    "\n",
    "\n",
    "\n",
    "ddata = surf_data_from_dense_cifti(dcifti, \"CIFTI_STRUCTURE_CORTEX_LEFT\", map_index = \"all\", fill_value = np.nan)\n",
    "\n",
    "\n",
    "views = ['medial']\n",
    "num_views = len(views)\n",
    "num_maps = 2\n",
    "mirror_views = True\n",
    "\n",
    "if mirror_views == True:\n",
    "    all_views = views + views[::-1]\n",
    "else:\n",
    "    all_views = views*2\n",
    "    \n",
    "w, h = plt.figaspect(num_maps / (num_views * 2))\n",
    "fig, axs = plt.subplots(num_maps,\n",
    "                        len(all_views),\n",
    "                        subplot_kw={'projection': '3d'},\n",
    "                        figsize=(w, h))\n",
    "for i, a in enumerate(axs.flat):\n",
    "\n",
    "        # Get row (map)\n",
    "        i_map = i // len(all_views)\n",
    "\n",
    "        # Get column\n",
    "        i_view = (i) % len(all_views)\n",
    "        #i_view = (i // (i_map + 1)) % num_views\n",
    "        view = all_views[i_view]\n",
    "\n",
    "        # Get hemisphere\n",
    "        hemi = i_view // num_views\n",
    "        if hemi == 0:\n",
    "            hemi = \"left\"\n",
    "        else:\n",
    "            hemi = \"right\"\n",
    "        print(f'{i} row/colum: {i_map}/{i_view} {view} {hemi}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calc_aligned_gradients is used to align the gradients that were created to a template. All the subject gradients are aligned \n",
    "with the HCP template, which in turn is aligned to the Margulies gradients from his 2016 paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import procrustes\n",
    "import nibabel as nib\n",
    "from brainspace.gradient import GradientMaps\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def calc_aligned_gradient(input_timeseries, template_gradients, kernel = 'cosine', alignment = 'procrustes'):\n",
    "    '''Align a set of gradients to a template series of gradients using procrustes, returns as dataframe\n",
    "\n",
    "    Loads GradientMaps, and makes same number of gradients as in the template\n",
    "    Fits the desired gradients to the reference template\n",
    "    Converts to pandas dataframe with labelled columns\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_timeseries (ndarray or list of arrays, shape = (n_ROIs, n_timepoints))\n",
    "    template_gradients, shape =  (n_rois, n_gradients)\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    grad_df_aligned\n",
    "        An array of values, rows x columns is ROIs x n number of gradients \n",
    "\n",
    "    ''' \n",
    "    # load GradientMaps\n",
    "    gm = GradientMaps(n_components=template_gradients.shape[1], random_state=0, alignment = 'procrustes', kernel=kernel)\n",
    "    # assert that the number of gradients requested does not exceed the regions of interest \n",
    "    # create the gradient as a matrix\n",
    "    gm.fit(input_timeseries, reference = template_gradients)\n",
    "    gradient = gm.aligned_\n",
    "    # convert gradient to a pandas dataframe\n",
    "    grad_df_aligned = pd.DataFrame(data = gradient, \n",
    "    columns=[f'grad{num + 1}' for num in range(gm.aligned_.shape[1])])\n",
    "    return grad_df_aligned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_df_aligned = calc_aligned_gradient(grad_matrix_df.values, margulies_grad_df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells are used to create a large file: all_concat which contains all information for all 438 subjects in the SPINS data.\n",
    "Subjects are separated by task using a filter, the subject ID is found via the file name, and thus as is the scanning site. The \n",
    "ROI labels are found from one of the original input nifty file headers. All this data is concatenated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_List_Of_Files(dir_Name):\n",
    "    '''Creates a list of files and sub directories from path\n",
    "    Creates empty list\n",
    "    Uses os to iterate over all subdirectories\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    dir_Name: str\n",
    "        Path to the directory containing desired files\n",
    "        \n",
    "    Returns \n",
    "    --------\n",
    "    List of all files in a given directory including subdirectories\n",
    "    ''' \n",
    "    # names in the given directory \n",
    "    list_Of_File = os.listdir(dir_Name)\n",
    "    allFiles = list()\n",
    "    # Iterate over all the entries\n",
    "    for entry in list_Of_File:\n",
    "        # Create full path\n",
    "        full_Path = os.path.join(dir_Name, entry)\n",
    "        # If entry is a directory then get the list of files in this directory \n",
    "        if os.path.isdir(full_Path):\n",
    "            allFiles = allFiles + get_List_Of_Files(full_Path)\n",
    "        else:\n",
    "            allFiles.append(full_Path)\n",
    "                \n",
    "    return allFiles\n",
    "\n",
    "path = '/scratch/a/arisvoin/edickie/SPINS_glasser_tian/2mm_noGSR'\n",
    "\n",
    "all_files = get_List_Of_Files(path)                        # get every file in directory of all subject files\n",
    "\n",
    "\n",
    "EA_filter = ['1D','RS_2mm','lock', 'history', 'Data', 'READ','ZHH0034','sub-CMP0180','sub-CMP0182','sub-CMP0191',\n",
    "             'sub-CMP0196','sub-CMP0198','sub-CMP0207','sub-CMP0213']                         # parameters to filter for only task emp files \n",
    "\n",
    "RS_filter = ['1D', 'EA_2mm_noGSR', 'lock', 'history', 'Data', 'READ','sub-CMP0180','sub-CMP0182','sub-CMP0191',\n",
    "             'sub-CMP0196','sub-CMP0198','sub-CMP0207','sub-CMP0213']                 # parameters to filter for only rest files\n",
    "\n",
    "EA_filter_data = [x for x in all_files if\n",
    "              all(y not in x for y in EA_filter)]          # creates a list of all the paths to EA data for all participants\n",
    "\n",
    "RS_filter_data = [x for x in all_files if\n",
    "              all(y not in x for y in RS_filter)]          # creates a list of all the paths to RS data for all participants\n",
    "\n",
    "\n",
    "EA_dfs = list()                               #list of all the pandas-read emp files\n",
    "for filename in EA_filter_data:\n",
    "     df = pd.read_csv(filename, header=None)\n",
    "     EA_dfs.append(df)\n",
    "    \n",
    "RS_dfs = list()                               #list of all the pandas-read rest files\n",
    "for filename in RS_filter_data:\n",
    "     df = pd.read_csv(filename, header=None)\n",
    "     RS_dfs.append(df)\n",
    "    \n",
    "EA_short_dfs = list()                         #creating smaller rest timepoint emp files to match rest file size \n",
    "\n",
    "for i in range(len(EA_dfs)):\n",
    "    EA_short_df = EA_dfs[i].loc[:, 0:208]\n",
    "    EA_short_dfs.append(EA_short_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsr_path = '/scratch/a/arisvoin/edickie/SPINS_glasser_tian/parcellated/2_mm'\n",
    "gsr_files = get_List_Of_Files(gsr_path)\n",
    "\n",
    "EAgsr_filter = ['rest','sub-CMP0180','sub-CMP0182','sub-CMP0191','sub-CMP0196','sub-CMP0198','sub-CMP0207','sub-CMP0213',\n",
    "                'sub-ZHH0034']\n",
    "RSgsr_filter = ['emp', 'sub-CMH0025','sub-CMH0044','sub-CMH0095','sub-CMH0138','sub-MRC0075', 'sub-MRP0077',\n",
    "                'sub-MRP0147','sub-MRP0149','sub-MRP0151','sub-MRP0161','sub-ZHH0038','sub-ZHP0061','sub-ZHP0086']\n",
    "\n",
    "EAgsr_filter_data = [x for x in gsr_files if\n",
    "              all(y not in x for y in EAgsr_filter)]\n",
    "\n",
    "RSgsr_filter_data = [x for x in gsr_files if\n",
    "              all(y not in x for y in RSgsr_filter)]\n",
    "\n",
    "EAgsr_subjects = list()            # empty list to hold all the subject IDs\n",
    "\n",
    "for i in range(len(EAgsr_filter_data)):      # getting subject IDs from directory above each file name\n",
    "    EAgsr_subjects_list = os.path.basename((os.path.dirname(EAgsr_filter_data[i])))\n",
    "    EAgsr_subjects.append(EAgsr_subjects_list)\n",
    "    \n",
    "RSgsr_subjects = list()\n",
    "\n",
    "for i in range(len(RSgsr_filter_data)):\n",
    "    RSgsr_subjects_list = os.path.basename((os.path.dirname(RSgsr_filter_data[i])))\n",
    "    RSgsr_subjects.append(RSgsr_subjects_list)\n",
    "    \n",
    "EA_gsr_dfs = list()                               #list of all the pandas-read emp files\n",
    "for filename in EAgsr_filter_data:\n",
    "     nib = nib.load(filename)\n",
    "     matrix = nib.get_fdata()\n",
    "     df = pd.DataFrame(matrix,\n",
    "                      columns=[f'grad{num + 1}' for num in range(df.shape[1])])\n",
    "     EA_gsr_dfs.append(df)\n",
    "    \n",
    "RS_gsr_dfs = list()                               #list of all the pandas-read rest files\n",
    "for filename in RSgsr_filter_data:\n",
    "     nib = nib.load(filename)\n",
    "     matrix = nib.get_fdata()\n",
    "     df = pd.DataFrame(matrix,\n",
    "                      columns=[f'grad{num + 1}' for num in range(df.shape[1])])\n",
    "     RS_gsr_dfs.append(df)\n",
    "    \n",
    "EA_gsr_short_dfs = list() \n",
    "\n",
    "for i in range(len(EA_dfs)):\n",
    "    EAgsr_short_df = EA_gsr_dfs[i].T.loc[:, 0:208]\n",
    "    EA_gsr_short_dfs.append(EAgsr_short_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_EA_gradients = list()            # empty list to hold the procrustes algined emp/rest/concatenated gradients \n",
    "\n",
    "for i in range(len(EA_dfs)):             # creates aligned emp/rest/concatenated gradients for all subjects\n",
    "    aligned = calc_aligned_gradient(EA_dfs[i].values, grad_df_aligned.values)\n",
    "    aligned_EA_gradients.append(aligned)\n",
    "    \n",
    "aligned_RS_gradients = list()\n",
    "\n",
    "for i in range(len(RS_dfs)):\n",
    "    aligned = calc_aligned_gradient(RS_dfs[i].values, grad_df_aligned.values)\n",
    "    aligned_RS_gradients.append(aligned)\n",
    "    \n",
    "\n",
    "aligned_concat_gradients = list()               \n",
    "concat_grad = list()\n",
    "\n",
    "for i in range(len(RS_dfs)):     \n",
    "    concat_grads = pd.concat([EA_dfs[i],RS_dfs[i]], axis=1)    #concatenates each subject's emp and rest gradients \n",
    "    concat_grad.append(concat_grads)\n",
    "    aligned = calc_aligned_gradient(concat_grad[i].values, grad_df_aligned.values)\n",
    "    aligned_concat_gradients.append(aligned)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_EAgsr_gradients = list()            # empty list to hold the procrustes algined emp/rest/concatenated gradients \n",
    "\n",
    "for i in range(len(EA_dfs)):             # creates aligned emp/rest/concatenated gradients for all subjects\n",
    "    aligned = calc_aligned_gradient(EA_gsr_dfs[i].T.values, grad_df_aligned.values)\n",
    "    aligned_EAgsr_gradients.append(aligned)\n",
    "    \n",
    "aligned_RSgsr_gradients = list()\n",
    "\n",
    "for i in range(len(RS_dfs)):\n",
    "    aligned = calc_aligned_gradient(RS_gsr_dfs[i].T.values, grad_df_aligned.values)\n",
    "    aligned_RSgsr_gradients.append(aligned)\n",
    "    \n",
    "\n",
    "aligned_concat_gsr_gradients = list()               \n",
    "concat_gsr_grad = list()\n",
    "\n",
    "for i in range(len(RS_dfs)):     \n",
    "    concat_grads = pd.concat([EA_gsr_dfs[i].T,RS_gsr_dfs[i].T], axis=1)    #concatenates each subject's emp and rest gradients \n",
    "    concat_gsr_grad.append(concat_grads)\n",
    "    aligned = calc_aligned_gradient(concat_gsr_grad[i].values, grad_df_aligned.values)\n",
    "    aligned_concat_gsr_gradients.append(aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_EA_short_gradients = list()             \n",
    "\n",
    "for i in range(len(EA_short_dfs)):             \n",
    "    aligned = calc_aligned_gradient(EA_short_dfs[i].values, grad_df_aligned.values)\n",
    "    aligned_EA_short_gradients.append(aligned)\n",
    "    \n",
    "aligned_concat_short_gradients = list()  \n",
    "concat_short_grad = list()\n",
    "\n",
    "for i in range(len(RS_dfs)):     \n",
    "    concat_grads = pd.concat([EA_short_dfs[i],RS_dfs[i]], axis=1)    #concatenates each subject's emp and rest gradients \n",
    "    concat_short_grad.append(concat_grads)\n",
    "    aligned = calc_aligned_gradient(concat_short_grad[i].values, grad_df_aligned.values)\n",
    "    aligned_concat_short_gradients.append(aligned) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_EAgsr_short_gradients = list()             \n",
    "\n",
    "for i in range(len(EA_short_dfs)):             \n",
    "    aligned = calc_aligned_gradient(EA_gsr_short_dfs[i].values, grad_df_aligned.values)\n",
    "    aligned_EAgsr_short_gradients.append(aligned)\n",
    "    \n",
    "aligned_short_concat_gsr_gradients = list()               \n",
    "concat_short_gsr_grad = list()\n",
    "\n",
    "for i in range(len(RS_dfs)):     \n",
    "    concat_grads = pd.concat([EA_gsr_short_dfs[i],RS_gsr_dfs[i].T], axis=1)    #concatenates each subject's emp and rest gradients \n",
    "    concat_short_gsr_grad.append(concat_grads)\n",
    "    aligned = calc_aligned_gradient(concat_short_gsr_grad[i].values, grad_df_aligned.values)\n",
    "    aligned_short_concat_gsr_gradients.append(aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "EA_subjects = list()            # empty list to hold all the subject IDs\n",
    "\n",
    "for i in range(len(EA_filter_data)):      # getting subject IDs from directory above each file name\n",
    "    EA_subjects_list = os.path.basename((os.path.dirname(EA_filter_data[i])))\n",
    "    EA_subjects.append(EA_subjects_list)\n",
    "    \n",
    "RS_subjects = list()\n",
    "\n",
    "for i in range(len(RS_filter_data)):\n",
    "    RS_subjects_list = os.path.basename((os.path.dirname(RS_filter_data[i])))\n",
    "    RS_subjects.append(RS_subjects_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Cifti2Image' object has no attribute 'cifti2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-21f215c7a5d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwrite_df_column_to_pscalar_nib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maligned_EA_gradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpconnGrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-5892caeb02ff>\u001b[0m in \u001b[0;36mwrite_df_column_to_pscalar_nib\u001b[0;34m(df, pscalar_template, to_filename, columns, labelname_column)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mpscalar_template\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpscalar_template\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mtemplate_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpscalar_template\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0maxis1_parcels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcifti2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcifti2_axes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParcelsAxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_index_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis1_parcels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcifti2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcifti2_axes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParcelsAxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Did not creat parcel axis\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Cifti2Image' object has no attribute 'cifti2'"
     ]
    }
   ],
   "source": [
    "write_df_column_to_pscalar_nib(aligned_EA_gradients[0], pconnGrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing every aligned gradient to a pscalar file, and in turn writing those to their subject folders in scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_EA_pscalars = list()       #writes every emp gradient to a pscalar\n",
    "\n",
    "for i in range (len(EA_subjects)):\n",
    "\n",
    "    aligned_EA_pscalar = write_df_column_to_pscalar_nib(aligned_EA_gradients[i], pconnGrad, to_filename= f'/scratch/a/arisvoin/lbassman/spins_gradients/{EA_subjects[i]}/{EA_subjects[i]}_task-emp_atlas-glassertian_gradients.pscalar.nii'\n",
    "                                                       )\n",
    "    aligned_EA_pscalars.append(aligned_EA_pscalar)\n",
    "    \n",
    "aligned_RS_pscalars = list()       #writes every rest gradient to a pscalar\n",
    "\n",
    "for i in range (len(RS_subjects)):\n",
    "\n",
    "    aligned_RS_pscalar = write_df_column_to_pscalar_nib(aligned_RS_gradients[i], pconnGrad, to_filename= f'/scratch/a/arisvoin/lbassman/spins_gradients/{RS_subjects[i]}/{RS_subjects[i]}_task-rest_atlas-glassertian_gradients.pscalar.nii'\n",
    "                                                       )\n",
    "    aligned_RS_pscalars.append(aligned_RS_pscalar)\n",
    "    \n",
    "aligned_concat_pscalars = list()    #writes every concatenated emp/rest gradient to a pscalar\n",
    "\n",
    "for i in range (len(RS_subjects)):\n",
    "\n",
    "    aligned_concat_pscalar = write_df_column_to_pscalar_nib(aligned_concat_gradients[i], pconnGrad, to_filename= f'/scratch/a/arisvoin/lbassman/spins_gradients/{RS_subjects[i]}/{RS_subjects[i]}_concatenated_atlas-glassertian_gradients.pscalar.nii'\n",
    "                                                       )\n",
    "    aligned_concat_pscalars.append(aligned_concat_pscalar)\n",
    "    \n",
    "aligned_EA_short_pscalars = list()       \n",
    "\n",
    "for i in range (len(EA_subjects)):\n",
    "\n",
    "    aligned_EA_short_pscalar = write_df_column_to_pscalar_nib(aligned_EA_short_gradients[i], pconnGrad, to_filename= f'/scratch/a/arisvoin/lbassman/spins_gradients/{EA_subjects[i]}/{EA_subjects[i]}_task-emp_shortened_atlas-glassertian_gradients.pscalar.nii'\n",
    "                                                       )\n",
    "    aligned_EA_short_pscalars.append(aligned_EA_short_pscalar)\n",
    "    \n",
    "aligned_concat_short_pscalars = list()    #writes every concatenated emp/rest gradient to a pscalar\n",
    "\n",
    "for i in range (len(RS_subjects)):\n",
    "\n",
    "    aligned_concat_short_pscalar = write_df_column_to_pscalar_nib(aligned_concat_short_gradients[i], pconnGrad, to_filename= f'/scratch/a/arisvoin/lbassman/spins_gradients/{RS_subjects[i]}/{RS_subjects[i]}_concatenated_shortened_atlas-glassertian_gradients.pscalar.nii'\n",
    "                                                       )\n",
    "    aligned_concat_short_pscalars.append(aligned_concat_short_pscalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_EAgsr_pscalars = list()       #writes every emp gradient to a pscalar\n",
    "\n",
    "for i in range (len(EA_subjects)):\n",
    "\n",
    "    aligned_EAgsr_pscalar = write_df_column_to_pscalar_nib(aligned_EAgsr_gradients[i], pconnGrad, to_filename= f'/scratch/a/arisvoin/lbassman/spins_gradients/{EA_subjects[i]}/{EA_subjects[i]}_task-emp_atlas-glassertian_gradients.pscalar.nii'\n",
    "                                                       )\n",
    "    aligned_EAgsr_pscalars.append(aligned_EAgsr_pscalar)\n",
    "    \n",
    "aligned_RSgsr_pscalars = list()       #writes every rest gradient to a pscalar\n",
    "\n",
    "for i in range (len(RS_subjects)):\n",
    "\n",
    "    aligned_RSgsr_pscalar = write_df_column_to_pscalar_nib(aligned_RSgsr_gradients[i], pconnGrad, to_filename= f'/scratch/a/arisvoin/lbassman/spins_gradients/{RS_subjects[i]}/{RS_subjects[i]}_task-rest_atlas-glassertian_gradients.pscalar.nii'\n",
    "                                                       )\n",
    "    aligned_RSgsr_pscalars.append(aligned_RSgsr_pscalar)\n",
    "    \n",
    "aligned_concatgsr_pscalars = list()    #writes every concatenated emp/rest gradient to a pscalar\n",
    "\n",
    "for i in range (len(RS_subjects)):\n",
    "\n",
    "    aligned_concat_gsr_pscalar = write_df_column_to_pscalar_nib(aligned_concat_gsr_gradients[i], pconnGrad, to_filename= f'/scratch/a/arisvoin/lbassman/spins_gradients/{RS_subjects[i]}/{RS_subjects[i]}_concatenated_atlas-glassertian_gradients.pscalar.nii'\n",
    "                                                       )\n",
    "    aligned_concat_gsr_pscalars.append(aligned_concat_gsr_pscalar)\n",
    "    \n",
    "aligned_EAgsr_short_pscalars = list()       \n",
    "\n",
    "for i in range (len(EA_subjects)):\n",
    "\n",
    "    aligned_EAgsr_short_pscalar = write_df_column_to_pscalar_nib(aligned_EAgsr_short_gradients[i], pconnGrad, to_filename= f'/scratch/a/arisvoin/lbassman/spins_gradients/{EA_subjects[i]}/{EA_subjects[i]}_task-emp_shortened_atlas-glassertian_gradients.pscalar.nii'\n",
    "                                                       )\n",
    "    aligned_EAgsr_short_pscalars.append(aligned_EAgsr_short_pscalar)\n",
    "    \n",
    "aligned_concat_gsr_short_pscalars = list()    #writes every concatenated emp/rest gradient to a pscalar\n",
    "\n",
    "for i in range (len(RS_subjects)):\n",
    "\n",
    "    aligned_concat_gsr_short_pscalar = write_df_column_to_pscalar_nib(aligned_concat_gsr_short_gradients[i], pconnGrad, to_filename= f'/scratch/a/arisvoin/lbassman/spins_gradients/{RS_subjects[i]}/{RS_subjects[i]}_concatenated_shortened_atlas-glassertian_gradients.pscalar.nii'\n",
    "                                                       )\n",
    "    aligned_concat_gsr_short_pscalars.append(aligned_concat_gsr_short_pscalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_list= list()               #empty list for the large concatenated file of all information\n",
    "subject_list_df = list()           #creates one column dataframe to be horizontally concat with file\n",
    "\n",
    "for i in range(len(EA_filter_data)):\n",
    "    subject=[EA_subjects[i]]*392\n",
    "    subject_list.append(subject)\n",
    "\n",
    "    subject_list_dfs=pd.DataFrame(subject_list[i],\n",
    "                                columns=['Subject'])\n",
    "    subject_list_df.append(subject_list_dfs)\n",
    "    \n",
    "ROIs = aligned_EA_pscalar.header.get_axis(1).name         #single column grad of the ROI for each value in the concat grad\n",
    "\n",
    "ROIs_list = pd.DataFrame(ROIs, \n",
    "                         columns=['ROI']) \n",
    "\n",
    "EA_list = ['EA'] * 392                  #specifies whether value in large gradient is rest or emp\n",
    "\n",
    "EA_list_df=pd.DataFrame(EA_list,\n",
    "                       columns=['Task'])\n",
    "\n",
    "RS_list = ['RS'] * 392\n",
    "\n",
    "RS_list_df=pd.DataFrame(RS_list,\n",
    "                       columns=['Task'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EA_concatenated = list()         #creates master file of all emp or rest files w subject ID, ROI, and EA/RS specified\n",
    "\n",
    "for i in range(len(EA_filter_data)):\n",
    "    EA_concat=pd.concat([aligned_EA_gradients[i],ROIs_list,EA_list_df,subject_list_df[i],networks_list_df],axis=1)\n",
    "    EA_concatenated.append(EA_concat)\n",
    "    \n",
    "EA_concat = pd.concat(EA_concatenated)\n",
    "\n",
    "RS_concatenated = list()\n",
    "\n",
    "for i in range(len(RS_filter_data)):\n",
    "    RS_concat=pd.concat([aligned_RS_gradients[i],ROIs_list,RS_list_df,subject_list_df[i],networks_list_df],axis=1)\n",
    "    RS_concatenated.append(RS_concat)\n",
    "    \n",
    "RS_concat = pd.concat(RS_concatenated)\n",
    "\n",
    "all_concat=pd.concat([EA_concat,RS_concat])        #vertically concatenates rest and emp dataframes to contain all data in one\n",
    "\n",
    "all_concat['Site'] = [s[4:7] for s in all_concat['Subject']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EAgsr_concatenated = list()         #creates master file of all emp or rest files w subject ID, ROI, and EA/RS specified\n",
    "\n",
    "for i in range(len(EA_filter_data)):\n",
    "    EAgsr_concat=pd.concat([aligned_EAgsr_gradients[i],ROIs_list,EA_list_df,subject_list_df[i],networks_list_df],axis=1)\n",
    "    EAgsr_concatenated.append(EAgsr_concat)\n",
    "    \n",
    "EAgsr_concat = pd.concat(EAgsr_concatenated)\n",
    "\n",
    "RSgsr_concatenated = list()\n",
    "\n",
    "for i in range(len(RS_filter_data)):\n",
    "    RSgsr_concat=pd.concat([aligned_RSgsr_gradients[i],ROIs_list,RS_list_df,subject_list_df[i],networks_list_df],axis=1)\n",
    "    RSgsr_concatenated.append(RSgsr_concat)\n",
    "    \n",
    "RSgsr_concat = pd.concat(RSgsr_concatenated)\n",
    "\n",
    "gsr_all_concat=pd.concat([EAgsr_concat,RSgsr_concat])        #vertically concatenates rest and emp dataframes to contain all data in one\n",
    "\n",
    "gsr_all_concat['Site'] = [s[4:7] for s in gsr_all_concat['Subject']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EA_short_concatenated = list()\n",
    "\n",
    "for i in range(len(EA_filter_data)):\n",
    "    EA_concat_s=pd.concat([aligned_EA_short_gradients[i],ROIs_list,EA_list_df,subject_list_df[i],networks_list_df],axis=1)\n",
    "    EA_short_concatenated.append(EA_concat_s)\n",
    "\n",
    "EA_short_concat = pd.concat(EA_short_concatenated)\n",
    "\n",
    "all_concat_shortened = pd.concat([EA_short_concat,RS_concat])\n",
    "all_concat_shortened['Site'] = [s[4:7] for s in all_concat['Subject']]\n",
    "\n",
    "EAgsr_short_concatenated = list()\n",
    "\n",
    "for i in range(len(EA_filter_data)):\n",
    "    EAgsr_concat_s=pd.concat([aligned_EAgsr_short_gradients[i],ROIs_list,EA_list_df,subject_list_df[i],networks_list_df],axis=1)\n",
    "    EAgsr_short_concatenated.append(EAgsr_concat_s)\n",
    "\n",
    "EAgsr_short_concat = pd.concat(EAgsr_short_concatenated)\n",
    "\n",
    "gsr_all_concat_shortened = pd.concat([EAgsr_short_concat,RSgsr_concat])\n",
    "gsr_all_concat_shortened['Site'] = [s[4:7] for s in gsr_all_concat['Subject']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/scratch/a/arisvoin/lbassman/spins_gradients/spins_concat_shortened'    #writes dataframe to csv in scratch \n",
    "\n",
    "all_concat_shortened.to_csv(path_or_buf=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/scratch/a/arisvoin/lbassman/spins_gradients/spins_concat_full'    #writes dataframe to csv in scratch \n",
    "\n",
    "all_concat.to_csv(path_or_buf=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_g = '/scratch/a/arisvoin/lbassman/spins_gradients/gsr_spins_concat_full'\n",
    "path_g_s = '/scratch/a/arisvoin/lbassman/spins_gradients/gsr_spins_concat_shortened'\n",
    "\n",
    "gsr_all_concat.to_csv(path_or_buf = path_g)\n",
    "gsr_all_concat_shortened.to_csv(path_or_buf = path_g_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folders are created in scratch to hold each subject and its data, as well as the concatenated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/scratch/a/arisvoin/lbassman/spins_gradients'    #creates folders for every subject ID\n",
    "EA_folders = EA_subjects\n",
    "for folder in EA_folders:\n",
    "    os.mkdir(os.path.join(root_path,folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_path = '/scratch/a/arisvoin/lbassman'      #creates sub-folder to hold all subject folders \n",
    "folder = 'spins_gradients'\n",
    "os.mkdir(os.path.join(root_path,folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the plotting function to plot one gradient for on subject as an example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting one gradient for one subject\n",
    "\n",
    "test_plot = plot_cifti_surf_montage(left_surf = '/scratch/a/arisvoin/edickie/SPINS_glasser_tian/tpl-fsLR/S1200.L.inflated_MSMAll.32k_fs_LR.surf.gii', \n",
    "                        right_surf = '/scratch/a/arisvoin/edickie/SPINS_glasser_tian/tpl-fsLR/S1200.R.inflated_MSMAll.32k_fs_LR.surf.gii', \n",
    "                        cifti_map = '/scratch/a/arisvoin/lbassman/spins_gradients/sub-MRC0057/sub-MRC0057_task-emp_atlas-glassertian_gradients.pscalar.nii', \n",
    "                        bg_cifti_map = None, \n",
    "                        bg_cifti_map_index = 0,\n",
    "                        colormap = \"RdYlBu\", \n",
    "                        darkness = 0,\n",
    "                        to_filename = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_plot.savefig('/scratch/a/arisvoin/lbassman/spins_gradients/sub-MRC0057/sub-MRC0057_task-emp-pic_atlas-glassertian_gradients.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a colour map based on values provided in the network file. Will be implemented into scatterplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_colours=([0,0,0,255], #subcortical, black\n",
    "                 [0,0,1,1], #visual1, blue\n",
    "                 [100/255,0,1,1], #visual2, lighter blue\n",
    "                 [0,1,1,1], #somatomotor, cyan\n",
    "                 [153/255,0,153/255,1], #cingulo-operator, purple\n",
    "                 [0,1,0,1], #dorsal attention, bright green\n",
    "                 [0,155/255,155/255,1], #language, azure blue\n",
    "                 [1,0,0,1], #frontoparietal, red\n",
    "                 [250/255,62/255,251/255,1], #auditory, pink/purple\n",
    "                 [1,1,0,1], # default 9, yellow\n",
    "                 [65/255,125/255,0,168/255], #orbito affective, green\n",
    "                 [1,157/255,0,1], # ventral multimodal, orange\n",
    "                 [177/255,89/255,40/255,1] # posterial multimodal, orange/brown\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "my_x_data = [0,1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "my_y_data = [0,1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "my_data = (my_x_data,my_y_data)\n",
    "plt.imshow(my_data, cmap=my_cmap)\n",
    "plt.colorbar()\n",
    "#plt.legend(labels='Visual1','Visual2','Somatomotor','Cingulo-Operator','Dorsal-Attention','Language','FrontoParietal',\n",
    " #          'Auditory','Default9','Orbito-Affective','Ventral-Multimodal',\"Posterior-Multimodal\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "my_cmap = LinearSegmentedColormap.from_list('network_basic', network_colours)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-nilearn_brainspace]",
   "language": "python",
   "name": "conda-env-.conda-nilearn_brainspace-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
