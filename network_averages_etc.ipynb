{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script will use the spins_concat etc. files created in the previous notebook (HCPgrad) to calculate the centroids (centre point) and dispersion around the centroid in every network, for every subject, by task. Functions are created to calculate each of these, and then are looped over every variable for the spins_concat etc. data that is grouped by subject, task, and network. The resulting data tables are then merged with a table containing the diagnostic information for each subject to faciliate stats done using that information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in files from HCPgrad_tidy_version notebook (now located in scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_concat = pd.read_csv('/scratch/a/arisvoin/lbassman/spins_gradients/spin_gradients/spins_concat_full.csv')\n",
    "merge_networks = pd.read_csv('/scratch/a/arisvoin/lbassman/spins_gradients/merge_networks')\n",
    "all_concat_short = pd.read_csv('/scratch/a/arisvoin/lbassman/spins_gradients/spins_concat_shortened.csv')\n",
    "gsr_all_concat = pd.read_csv('/scratch/a/arisvoin/lbassman/spins_gradients/gsr_spins_concat_full.csv')\n",
    "gsr_all_concat_short = pd.read_csv('/scratch/a/arisvoin/lbassman/spins_gradients/gsr_spins_concat_shortened.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping the all_concat file by site, task, and subbject, and the merge_networks file by network to create a list of each of the subject IDs, networks, sites and tasks of all the spins data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_group = all_concat.groupby(['Subject'])            #grouping_by separates dataframe by column - creates indiv. groups\n",
    "network_group = merge_networks.groupby(['network1'])\n",
    "site_group = all_concat.groupby(['Site'])\n",
    "task_group = all_concat.groupby(['Task'])\n",
    "\n",
    "list_subjects = list(subject_group.groups.keys())         #groups.keys gets all groupby group names, creates list\n",
    "list_networks = list(network_group.groups.keys())\n",
    "list_sites = list(site_group.groups.keys())\n",
    "list_tasks = list(task_group.groups.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating centroid function to determine the centroid for each network given specified subject id and task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_centroid(file, grouping_by, subject, task, network):\n",
    "    '''Calculates the centroid of a network by subject and by task for given parameters in a file\n",
    "    Takes first three gradients only\n",
    "    Uses pandas groupby function to separate by given group, column in the file\n",
    "    Takes one object in the given group each to specify which element in the group\n",
    "    Calculates mean for each gradient and outputs as a 3D coordinate point.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file: str or path\n",
    "        csv containing all information\n",
    "    grouping_by: str \n",
    "        column in file \n",
    "    subject: str\n",
    "    task: str\n",
    "    network: str\n",
    "        Specify which network, subject, and task pandas will get from the group\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_mean, y_mean, z_mean\n",
    "        A three point coordinate, representing the centroid of the network (by subject and task)\n",
    "    '''\n",
    "    grouped_by = file.groupby(grouping_by)\n",
    "    #create group, and then get groups based on parameters\n",
    "    group_name = grouped_by.get_group((subject, task, network))\n",
    "    #calculate mean of each gradient\n",
    "    x_mean = group_name['grad1'].mean()\n",
    "    y_mean = group_name['grad2'].mean()\n",
    "    z_mean = group_name['grad3'].mean()\n",
    "    return x_mean, y_mean, z_mean\n",
    "\n",
    "                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dispersion function to determine the average dispersion around the centroid for each network given specified subject id and task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calc_dispersion(file, grouping_by, subject, task, network):\n",
    "    '''Calculates the dispersion around a centroid of a network by subject and by task for given parameters in a file\n",
    "    Takes first three gradients only\n",
    "    Uses pandas groupby function to separate by given group, column in the file\n",
    "    Takes one object in the given group each to specify which element in the group\n",
    "    Calculates dispersion for each gradient and outputs value. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file: str or path\n",
    "        csv containing all information\n",
    "    grouping_by: str \n",
    "        column in file \n",
    "    subject: str\n",
    "    task: str\n",
    "    network: str\n",
    "        Specify which network, subject, etc pandas will get from the group\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dispersion\n",
    "        A single value, representing the average dispersion of the network (by subject and task)\n",
    "    '''\n",
    "    grouped_by = file.groupby(grouping_by)\n",
    "    group_name = grouped_by.get_group((subject, task, network))\n",
    "    #calculate dispersion by using euclidean distance formula and standard deviation of each point of each gradient\n",
    "    dispersion = np.sqrt((group_name['grad1'].std()**2)+(group_name['grad2'].std()**2)+(group_name['grad3'].std()**2))\n",
    "    return dispersion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pseudo nested for loop to calculate the dispersion of every network for every subject and each task which is appended to an empty dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "dispersion_total = []\n",
    "\n",
    "for i,j,k in itertools.product(range(len(list_subjects)), range(len(list_tasks)), range(len(list_networks))):\n",
    "    dispersion =  calc_dispersion(all_concat, ['Subject','Task','Network'], list_subjects[i], list_tasks[j], list_networks[k])\n",
    "    dispersion_total.append(dispersion)\n",
    "    \n",
    "dispersion_total_short = []\n",
    "\n",
    "for i,j,k in itertools.product(range(len(list_subjects)), range(len(list_tasks)), range(len(list_networks))):\n",
    "    dispersion_short =  calc_dispersion(all_concat_short, ['Subject','Task','Network'], list_subjects[i], list_tasks[j], list_networks[k])\n",
    "    dispersion_total_short.append(dispersion_short)\n",
    "    \n",
    "dispersion_total_gsr = []\n",
    "\n",
    "for i,j,k in itertools.product(range(len(list_subjects)), range(len(list_tasks)), range(len(list_networks))):\n",
    "    dispersion_gsr =  calc_dispersion(gsr_all_concat, ['Subject','Task','Network'], list_subjects[i], list_tasks[j], list_networks[k])\n",
    "    dispersion_total_gsr.append(dispersion_gsr)\n",
    "    \n",
    "dispersion_total_gsr_short = []\n",
    "\n",
    "for i,j,k in itertools.product(range(len(list_subjects)), range(len(list_tasks)), range(len(list_networks))):\n",
    "    dispersion_gsr_short =  calc_dispersion(gsr_all_concat_short, ['Subject','Task','Network'], list_subjects[i], list_tasks[j], list_networks[k])\n",
    "    dispersion_total_gsr_short.append(dispersion_gsr_short)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same method used to calcualte centroid, one column dataframe is outputted to hold mean value for each dimension (gradient) as a coordinate (x,y,z) --> (grad1,grad2,grad3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_total = []\n",
    "\n",
    "for i,j,k in itertools.product(range(len(list_subjects)), range(len(list_tasks)), range(len(list_networks))):\n",
    "    centroid =  calc_centroid(all_concat, ['Subject','Task','Network'], list_subjects[i], list_tasks[j], list_networks[k])\n",
    "    centroid_total.append(centroid)\n",
    "    \n",
    "centroid_total_short = []\n",
    "\n",
    "for i,j,k in itertools.product(range(len(list_subjects)), range(len(list_tasks)), range(len(list_networks))):\n",
    "    centroid_short =  calc_centroid(all_concat_short, ['Subject','Task','Network'], list_subjects[i], list_tasks[j], list_networks[k])\n",
    "    centroid_total_short.append(centroid_short)\n",
    "    \n",
    "centroid_total_gsr = []\n",
    "\n",
    "for i,j,k in itertools.product(range(len(list_subjects)), range(len(list_tasks)), range(len(list_networks))):\n",
    "    centroid_gsr =  calc_centroid(gsr_all_concat, ['Subject','Task','Network'], list_subjects[i], list_tasks[j], list_networks[k])\n",
    "    centroid_total_gsr.append(centroid_gsr)\n",
    "\n",
    "centroid_total_gsr_short = []\n",
    "\n",
    "for i,j,k in itertools.product(range(len(list_subjects)), range(len(list_tasks)), range(len(list_networks))):\n",
    "    centroid_gsr_short =  calc_centroid(gsr_all_concat_short, ['Subject','Task','Network'], list_subjects[i], list_tasks[j], list_networks[k])\n",
    "    centroid_total_gsr_short.append(centroid_gsr_short)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the first dataframe using the full, no gsr data. A full list of subjects is created, and a task list using separate lists for EA and rest, which are converted into dataframes to be concat into a larger file. Columns are then created for dispersion and centroid using the outputs from the function loops, and a site coclumn is created from the subject ID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_list= list()               \n",
    "subject_list_df = list()          \n",
    "\n",
    "for i in range(len(list_subjects)):\n",
    "    subject=[list_subjects[i]]*26\n",
    "    subject_list.append(subject)\n",
    "\n",
    "    subject_list_dfs=pd.DataFrame(subject_list[i],\n",
    "                                columns=['Subject'])\n",
    "    subject_list_df.append(subject_list_dfs)\n",
    "    \n",
    "EA_list = ['EA'] * 13                  \n",
    "RS_list = ['RS'] * 13 \n",
    "task_list = EA_list + RS_list   \n",
    "task_list_df = pd.DataFrame(task_list,\n",
    "                           columns = ['Task']) \n",
    "\n",
    "network_list = list_networks*2\n",
    "networks_df = pd.DataFrame(network_list, \n",
    "                         columns=['Network'])\n",
    "\n",
    "full_concatenated = list()         \n",
    "\n",
    "for i in range(len(list_subjects)):\n",
    "    concat=pd.concat([task_list_df,subject_list_df[i],networks_df],axis=1)\n",
    "    full_concatenated.append(concat)\n",
    "    \n",
    "total_concat = pd.concat(full_concatenated)\n",
    "\n",
    "total_concat['Dispersion'] = dispersion_total \n",
    "total_concat['Centroid']=centroid_total\n",
    "total_concat['Site'] = [s[4:7] for s in total_concat['Subject']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same dfs for subject, task, site used. Dispersion and centroid added for the full data with gsr applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsr_total_concat = pd.concat(full_concatenated)\n",
    "\n",
    "gsr_total_concat['Dispersion']=dispersion_total_gsr\n",
    "gsr_total_concat['Centroid']=centroid_total_gsr\n",
    "gsr_total_concat['Site'] = [s[4:7] for s in gsr_total_concat['Subject']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same method for the shortened, no gsr data, and shortened gsr data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_concat_short = pd.concat(full_concatenated)\n",
    "\n",
    "total_concat_short['Dispersion']=dispersion_total_short\n",
    "total_concat_short['Centroid']=centroid_total_short\n",
    "total_concat_short['Site'] = [s[4:7] for s in total_concat_short['Subject']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsr_total_concat_short = pd.concat(full_concatenated)\n",
    "\n",
    "gsr_total_concat_short['Dispersion']=dispersion_total_gsr_short\n",
    "gsr_total_concat_short['Centroid']=centroid_total_gsr_short\n",
    "gsr_total_concat_short['Site'] = [s[4:7] for s in gsr_total_concat_short['Subject']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in file containing diagnosis for each subject, to be merged to the total_concat files to include diagnostic information. Merging the data with each of the files, and writing them to scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_df = pd.read_csv('/scratch/a/arisvoin/lbassman/spins_gradients/spin_gradients/diagnostic_information.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_concat_merge = pd.merge(total_concat, diagnostic_df, how = \"left\", left_on = \"Subject\", right_on = \"subject\")\n",
    "total_concat_short_merge = pd.merge(total_concat_short, diagnostic_df, how = \"left\", left_on = \"Subject\", right_on = \"subject\")\n",
    "gsr_total_concat_merge = pd.merge(gsr_total_concat, diagnostic_df, how = \"left\", left_on = \"Subject\", right_on = \"subject\")\n",
    "gsr_total_concat_short_merge = pd.merge(gsr_total_concat_short, diagnostic_df, how = \"left\", left_on = \"Subject\", right_on = \"subject\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/scratch/a/arisvoin/lbassman/spins_gradients/network_averages_full.csv'\n",
    "path_s = '/scratch/a/arisvoin/lbassman/spins_gradients/network_averages_shortened.csv'\n",
    "path_g = '/scratch/a/arisvoin/lbassman/spins_gradients/gsr_network_averages_full.csv'\n",
    "path_g_s = '/scratch/a/arisvoin/lbassman/spins_gradients/gsr_network_averages_shortened.csv'\n",
    "\n",
    "total_concat_merge.to_csv(path_or_buf=path)    \n",
    "total_concat_short_merge.to_csv(path_or_buf=path_s)\n",
    "gsr_total_concat_merge.to_csv(path_or_buf=path_g)\n",
    "gsr_total_concat_short_merge.to_csv(path_or_buf=path_g_s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-nilearn_brainspace]",
   "language": "python",
   "name": "conda-env-.conda-nilearn_brainspace-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
